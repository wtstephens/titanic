{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://www.intrepidstrategic.com/wp-content/uploads/2023/09/titanic.jpg)","metadata":{}},{"cell_type":"markdown","source":"## A study in feature engineering","metadata":{}},{"cell_type":"markdown","source":"### This notebook takes a segmentation approach\n* Segment by name country of origin, as England was relatively homogenous in the early 1900s\n* Bin name titles by kind (ie: nobility titles, professional titles, etc)\n* Identify likely parents of various ages based on historic data\n* Identify & bin by group size\n* Normalize fare & bin by sixths\n* Age binning","metadata":{}},{"cell_type":"markdown","source":"## The local Anaconda version of this notebook landed in the TOP 13%!\nKaggle won't run the fine tuning in its entirety. Currently, I am trying to identify why the Anaconda version performs quite well regularly while the Kaggle version does not. I'm still in the early learning phase of ML and am eager to hear your feedback. ","metadata":{}},{"cell_type":"markdown","source":"## COMMENTS ARE WELCOME!\nI'm here to learn! ","metadata":{}},{"cell_type":"code","source":"!pip install dataprep","metadata":{"_uuid":"0b7ab3c3-b00f-44fe-90ad-c1938a16b24a","_cell_guid":"2034b8b2-4383-40e9-b405-761dc512d051","collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:02.001453Z","iopub.execute_input":"2023-10-02T16:16:02.001811Z","iopub.status.idle":"2023-10-02T16:16:13.379710Z","shell.execute_reply.started":"2023-10-02T16:16:02.001782Z","shell.execute_reply":"2023-10-02T16:16:13.378682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import what we'll need","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, roc_auc_score\nimport plotly.express as px\nimport numpy as np \nimport pandas as pd \nimport scipy\nfrom scipy.stats import mode\nimport shap\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.impute import IterativeImputer\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom xgboost import cv\nimport xgboost as xgb\nfrom dataprep.eda import create_report\nfrom IPython.display import display\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"d022aaeb-7830-4a1b-bc39-f7c48f78cc22","_cell_guid":"80972bb3-b56a-44ce-9edf-75cb7f3a18f0","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-10-02T16:16:13.381785Z","iopub.execute_input":"2023-10-02T16:16:13.382178Z","iopub.status.idle":"2023-10-02T16:16:17.891246Z","shell.execute_reply.started":"2023-10-02T16:16:13.382118Z","shell.execute_reply":"2023-10-02T16:16:17.890361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Examine the data","metadata":{"_uuid":"648a9e19-57da-47e8-b74d-435f23700f0b","_cell_guid":"f5cac1c0-063b-4863-bf60-fbbcfc89a1fe","trusted":true}},{"cell_type":"code","source":"# Read the train_data\ntrain_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndisplay(train_data.head())\n\n# Read the test_data\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndisplay(test_data.head())\n\n# Display the information of test_data\ndisplay(test_data.info())","metadata":{"_uuid":"b12c0121-7388-454f-8ca4-902d5770b847","_cell_guid":"d240b8eb-f052-4797-b63f-9f775b89f00f","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:17.892645Z","iopub.execute_input":"2023-10-02T16:16:17.892972Z","iopub.status.idle":"2023-10-02T16:16:17.943746Z","shell.execute_reply.started":"2023-10-02T16:16:17.892944Z","shell.execute_reply":"2023-10-02T16:16:17.942931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataprep Create Report","metadata":{}},{"cell_type":"code","source":"create_report(train_data)","metadata":{"_uuid":"7bf3b2a6-b2af-4e09-8e30-444ce66dc710","_cell_guid":"997ffcfc-8aee-4310-8451-5cb1639d6564","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:17.945151Z","iopub.execute_input":"2023-10-02T16:16:17.945774Z","iopub.status.idle":"2023-10-02T16:16:26.998552Z","shell.execute_reply.started":"2023-10-02T16:16:17.945745Z","shell.execute_reply":"2023-10-02T16:16:26.997352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find null values","metadata":{"_uuid":"5fd7a7ef-d4a9-4c0a-87a0-d01005fb3f0b","_cell_guid":"9df3f8d5-29f9-4650-a73b-0f5aa353d711","trusted":true}},{"cell_type":"code","source":"test_data_df = pd.DataFrame(test_data.isnull().sum())\nprint(test_data_df)","metadata":{"_uuid":"1d70f582-99cf-41bf-808c-2f6d2d0f056d","_cell_guid":"93f24af7-3dd2-491c-84f5-4a3647c51013","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.001141Z","iopub.execute_input":"2023-10-02T16:16:27.001997Z","iopub.status.idle":"2023-10-02T16:16:27.015284Z","shell.execute_reply.started":"2023-10-02T16:16:27.001962Z","shell.execute_reply":"2023-10-02T16:16:27.013896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_df = pd.DataFrame(train_data.isnull().sum())\nprint(train_data_df)","metadata":{"_uuid":"a0c36c00-d1be-4625-87e1-c4860b51c83e","_cell_guid":"557d84c4-b7b3-4f2e-af8d-4975c9043a43","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.017575Z","iopub.execute_input":"2023-10-02T16:16:27.018095Z","iopub.status.idle":"2023-10-02T16:16:27.045676Z","shell.execute_reply.started":"2023-10-02T16:16:27.018058Z","shell.execute_reply":"2023-10-02T16:16:27.044637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n## Change the null cabin to a category 'Unknown' \nWe can see in the data that a huge chunk of cabins are listed as 'null,' yet I am unsure as to why. It's possible the nulls could be for a reason, so we'll add them into a category of their own.","metadata":{"_uuid":"fbb9bc78-4299-4258-9a69-38e8dcc7f69a","_cell_guid":"0e48e172-24ad-442a-9dd5-6beaf288b181","trusted":true}},{"cell_type":"code","source":"# Find and replace null values in train_data for 'Cabin' and 'Ticket'\ntrain_data['Cabin'].fillna('Unknown', inplace=True)\ntrain_data['Ticket'].fillna('Unknown', inplace=True)\n\n# Find and replace null values in test_data for 'Cabin' and 'Ticket'\ntest_data['Cabin'].fillna('Unknown', inplace=True)\ntest_data['Ticket'].fillna('Unknown', inplace=True)","metadata":{"_uuid":"0bfce894-7dba-479c-97d6-0cf5bf04a45b","_cell_guid":"47455023-8acf-4598-8026-67049c988a51","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.047030Z","iopub.execute_input":"2023-10-02T16:16:27.047996Z","iopub.status.idle":"2023-10-02T16:16:27.064962Z","shell.execute_reply.started":"2023-10-02T16:16:27.047955Z","shell.execute_reply":"2023-10-02T16:16:27.063890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Take a look at ages","metadata":{}},{"cell_type":"code","source":"unique_ages = train_data['Age'].unique()\nprint(unique_ages)","metadata":{"_uuid":"df0aae68-e949-470c-8cdc-9b845f177673","_cell_guid":"f185fb09-2dc2-402c-9112-fd69be780676","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.066542Z","iopub.execute_input":"2023-10-02T16:16:27.067818Z","iopub.status.idle":"2023-10-02T16:16:27.087416Z","shell.execute_reply.started":"2023-10-02T16:16:27.067771Z","shell.execute_reply":"2023-10-02T16:16:27.086185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### We'll use predictive modeling to fill ages in before running final prediction","metadata":{"_uuid":"485b5678-72e1-46f5-adaa-c8ccbf210836","_cell_guid":"be254ded-6000-4f13-a938-12838c35df50","trusted":true}},{"cell_type":"markdown","source":"\n## Let's turn these floats off.","metadata":{"_uuid":"80966b08-44c4-4e74-aaff-07084ce53b1f","_cell_guid":"6b3af47a-64ca-4a59-966f-5a8722026748","trusted":true}},{"cell_type":"markdown","source":"And convert Age to numeric","metadata":{"_uuid":"24cc1a6b-b298-43aa-a24c-d0ee93d9554e","_cell_guid":"2462fa37-fddb-42d5-b196-a03bc388e2b5","trusted":true}},{"cell_type":"code","source":"# Convert the Age column to numeric (including NaN values)\ntrain_data['Age'] = pd.to_numeric(train_data['Age'], errors='coerce')\ntest_data['Age'] = pd.to_numeric(test_data['Age'], errors='coerce')\n\n# Round the float values and then convert to integers\ntrain_data['Age'] = train_data['Age'].round().astype('Int64')\ntest_data['Age'] = test_data['Age'].round().astype('Int64')","metadata":{"_uuid":"f58c29fa-e03d-45b1-9f8a-2cd6daa3ca2c","_cell_guid":"1e7ceb23-dcb3-4b6a-b116-cafbe83c1a6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.088860Z","iopub.execute_input":"2023-10-02T16:16:27.089282Z","iopub.status.idle":"2023-10-02T16:16:27.104034Z","shell.execute_reply.started":"2023-10-02T16:16:27.089245Z","shell.execute_reply":"2023-10-02T16:16:27.102024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### now we'll view that again","metadata":{"_uuid":"1c3e980c-ab47-4198-9032-81bd753f4507","_cell_guid":"98572374-1bac-45cb-8c6c-45b76c03848c","trusted":true}},{"cell_type":"code","source":"unique_ages = train_data['Age'].unique()\nprint(unique_ages)","metadata":{"_uuid":"95dd5d72-2dea-4358-b10a-6e0b106d2b87","_cell_guid":"47735657-7810-49c9-9a6a-0e76b24bc787","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.105715Z","iopub.execute_input":"2023-10-02T16:16:27.106439Z","iopub.status.idle":"2023-10-02T16:16:27.115537Z","shell.execute_reply.started":"2023-10-02T16:16:27.106391Z","shell.execute_reply":"2023-10-02T16:16:27.114398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quickly view unique values\n#### Make a dataframe with unique options in each column","metadata":{"_uuid":"4175b66a-4249-41af-9a6b-db49f3ce7abe","_cell_guid":"517e4cd3-d6f2-4977-837b-6e0d0abf443f","trusted":true}},{"cell_type":"code","source":"unique_values_dict = {}\n\n# Exclude 'PassengerID' column\ncolumns_to_check = [col for col in train_data.columns if col != 'PassengerID']\n\n# Find the maximum number of unique values across columns\nmax_unique_values = max([len(train_data[col].dropna().unique()) for col in columns_to_check])\n\n# Populate unique_values_dict\nfor col in columns_to_check:\n    unique_values = train_data[col].dropna().unique()\n    padding = max_unique_values - len(unique_values)\n    unique_values_dict[col] = list(unique_values) + [''] * padding\n\n# Create DataFrame\nunique_df = pd.DataFrame(unique_values_dict)\n\nunique_df.head(100)","metadata":{"_uuid":"feafab3c-76a3-40b6-85aa-c6050c29ebfb","_cell_guid":"0edba36b-0a09-4718-a4ba-ab39ddfeaf58","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:27.117177Z","iopub.execute_input":"2023-10-02T16:16:27.117582Z","iopub.status.idle":"2023-10-02T16:16:27.154838Z","shell.execute_reply.started":"2023-10-02T16:16:27.117552Z","shell.execute_reply":"2023-10-02T16:16:27.154085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Take a look at all abbreviations in the 'Name' column","metadata":{"_uuid":"784678c6-7ed9-4061-aae4-f3607dcd11c4","_cell_guid":"a92c9ac5-71b1-47a4-bb7a-fe648c0f576d","trusted":true}},{"cell_type":"code","source":"# Extract words that end with a period from the 'Name' column\ntrain_data['Title'] = train_data['Name'].str.extract('([A-Za-z]+\\.)', expand=False)\n# Extract words that end with a period from the 'Name' column\ntest_data['Title'] = test_data['Name'].str.extract('([A-Za-z]+\\.)', expand=False)","metadata":{"_uuid":"2a36cc4a-be0a-49f2-859f-89ad455ae763","_cell_guid":"ec095da2-7e26-4920-a89f-cbf347c1919a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.156224Z","iopub.execute_input":"2023-10-02T16:16:27.157268Z","iopub.status.idle":"2023-10-02T16:16:27.166408Z","shell.execute_reply.started":"2023-10-02T16:16:27.157237Z","shell.execute_reply":"2023-10-02T16:16:27.165232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['Title'].unique()","metadata":{"_uuid":"0462bcec-763d-4373-af9c-3916b800cd5b","_cell_guid":"2b2e49eb-374f-4caa-84fb-7d709b769eaa","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.167677Z","iopub.execute_input":"2023-10-02T16:16:27.167971Z","iopub.status.idle":"2023-10-02T16:16:27.177201Z","shell.execute_reply.started":"2023-10-02T16:16:27.167947Z","shell.execute_reply":"2023-10-02T16:16:27.176388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-hot encode titles","metadata":{"_uuid":"ebee2435-cbfa-418d-a364-96ab95d67932","_cell_guid":"574a9a73-3a77-41e9-9040-223720e89f76","trusted":true}},{"cell_type":"markdown","source":"#### Mr.\nGrown Men\n#### Master.\nMen under 18\n#### MsMlle\nWomen who are unmarried\n#### MrsMme\nWomen who are married\n#### Nobility\nMen and women with nobility titles\n#### ProTitle\nPeople with a title signifying their profession\n#### Military\nPeople with a military title","metadata":{}},{"cell_type":"code","source":"# Initialize new columns with zero\ntrain_data[['MsMlle', 'MrsMme', 'Nobility', 'ProTitle', 'Military']] = 0\ntest_data[['MsMlle', 'MrsMme', 'Nobility', 'ProTitle', 'Military']] = 0\n\n# Define combined titles and their respective column names\ncombined_titles = {\n    'MsMlle': ['Ms.', 'Mlle.'],\n    'MrsMme': ['Mrs.', 'Mme.'],\n    'Nobility': ['Don.', 'Sir.', 'Lady.', 'Jonkheer.'],\n    'ProTitle': ['Rev.', 'Dr.'],\n    'Military': ['Major.', 'Col.', 'Capt.']\n}\n\n# Apply the changes to train_data and test_data\nfor column_name, titles in combined_titles.items():\n    for title in titles:\n        train_data.loc[train_data['Name'].str.contains(title), column_name] = 1\n        test_data.loc[test_data['Name'].str.contains(title), column_name] = 1\n        \ntitles = ['Mr.', 'Master.']","metadata":{"_uuid":"92b2c8ba-c1ae-4203-a6f0-4f617330a8d7","_cell_guid":"2caaa2d7-18e2-446a-a57e-fadb7c015460","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.181255Z","iopub.execute_input":"2023-10-02T16:16:27.182254Z","iopub.status.idle":"2023-10-02T16:16:27.214693Z","shell.execute_reply.started":"2023-10-02T16:16:27.182220Z","shell.execute_reply":"2023-10-02T16:16:27.213892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for title in titles:\n    train_data[title] = train_data['Name'].apply(lambda x: 1 if title in x else 0)\n    test_data[title] = test_data['Name'].apply(lambda x: 1 if title in x else 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:27.216074Z","iopub.execute_input":"2023-10-02T16:16:27.216449Z","iopub.status.idle":"2023-10-02T16:16:27.225589Z","shell.execute_reply.started":"2023-10-02T16:16:27.216411Z","shell.execute_reply":"2023-10-02T16:16:27.224549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove the titles from the names before we move on","metadata":{"_uuid":"738030dd-7155-4c39-8234-810fac144a39","_cell_guid":"fe35e257-eb1e-4004-9d55-b5917d42e8c9","trusted":true}},{"cell_type":"code","source":"titles_to_check = ['Mr.', 'Mrs.', 'Miss.', 'Master.', 'Don.', 'Rev.', 'Dr.', 'Mme.', 'Ms.', 'Major.', 'Lady.', 'Sir.', 'Mlle.', 'Col.', 'Capt.', 'Countess.', 'Jonkheer.']\n\n# Define a function to remove the titles\ndef remove_titles_from_name(df, titles):\n    for title in titles:\n        df['Name'] = df['Name'].str.replace(title, '', regex=False)\n    return df\n\n# Apply the function to both train_data and test_data\ntrain_data = remove_titles_from_name(train_data, titles_to_check)\ntest_data = remove_titles_from_name(test_data, titles_to_check)","metadata":{"_uuid":"b0c248c3-981c-499d-982a-8781ba66a866","_cell_guid":"1215f668-4454-42a8-b8f9-3827933cc496","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.226723Z","iopub.execute_input":"2023-10-02T16:16:27.227272Z","iopub.status.idle":"2023-10-02T16:16:27.254469Z","shell.execute_reply.started":"2023-10-02T16:16:27.227242Z","shell.execute_reply":"2023-10-02T16:16:27.253629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's in a name?\n### a hypothesis\n\nSurname origin may very well be important, as it could demonstrate those who travelled from a nearby country to embark on The Titanic. The fated ship set sail from Southhampton in southern England in 1912. In the 1901 census, 96% of the population was born in England or Wales in Great Britain. If you happen to find census data, either tabular or raw images then please let me know so I can gain a more accurate understanding of the surname breakdown of English residents.\n\n### Import dataframes & assign country\n[from Surname Language of Origin](https://www.kaggle.com/datasets/sinclairg/surname-language-of-origin/) by Sinclair","metadata":{"_uuid":"97a5db6f-199d-49f4-9120-77edc83bbfb7","_cell_guid":"61ced8cb-7cf9-4097-9ed8-3f8023588f58","trusted":true}},{"cell_type":"code","source":"def load_txt_to_df(filepath):\n    with open(filepath, 'r') as f:\n        lines = f.readlines()\n    df = pd.DataFrame(lines, columns=['Surname'])\n    df['Surname'] = df['Surname'].str.strip()\n    return df\nfilepaths = {\n    \"arabic\": \"/kaggle/input/surname-language-of-origin/data/names/Arabic.txt\",\n    \"chinese\": \"/kaggle/input/surname-language-of-origin/data/names/Chinese.txt\",\n    \"czech\": \"/kaggle/input/surname-language-of-origin/data/names/Czech.txt\",\n    \"dutch\": \"/kaggle/input/surname-language-of-origin/data/names/Dutch.txt\",\n    \"english\": \"/kaggle/input/surname-language-of-origin/data/names/English.txt\",\n    \"french\": \"/kaggle/input/surname-language-of-origin/data/names/French.txt\",\n    \"german\": \"/kaggle/input/surname-language-of-origin/data/names/German.txt\",\n    \"greek\": \"/kaggle/input/surname-language-of-origin/data/names/Greek.txt\",\n    \"irish\": \"/kaggle/input/surname-language-of-origin/data/names/Irish.txt\",\n    \"italian\": \"/kaggle/input/surname-language-of-origin/data/names/Italian.txt\",\n    \"japanese\": \"/kaggle/input/surname-language-of-origin/data/names/Japanese.txt\",\n    \"korean\": \"/kaggle/input/surname-language-of-origin/data/names/Korean.txt\",\n    \"polish\": \"/kaggle/input/surname-language-of-origin/data/names/Polish.txt\",\n    \"portuguese\": \"/kaggle/input/surname-language-of-origin/data/names/Portuguese.txt\",\n    \"russian\": \"/kaggle/input/surname-language-of-origin/data/names/Russian.txt\",\n    \"scottish\": \"/kaggle/input/surname-language-of-origin/data/names/Scottish.txt\",\n    \"spanish\": \"/kaggle/input/surname-language-of-origin/data/names/Spanish.txt\",\n    \"vietnamese\": \"/kaggle/input/surname-language-of-origin/data/names/Vietnamese.txt\",\n}\n\ndataframes = {}\nfor name, path in filepaths.items():\n    dataframes[name] = load_txt_to_df(path)","metadata":{"_uuid":"194cd2bf-289b-4f76-acea-9539fe70a5f9","_cell_guid":"ae663bc6-d918-441a-ba45-5c4100329b84","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.255716Z","iopub.execute_input":"2023-10-02T16:16:27.256605Z","iopub.status.idle":"2023-10-02T16:16:27.321035Z","shell.execute_reply.started":"2023-10-02T16:16:27.256575Z","shell.execute_reply":"2023-10-02T16:16:27.320157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-hot encode names to country of origin","metadata":{"_uuid":"76b487ba-8834-4619-a6b2-aa09ca6662e3","_cell_guid":"6ad722f2-e356-4ab1-829c-9b6075de072c","trusted":true}},{"cell_type":"code","source":"# Create columns for each origin in train_data and test_data\norigins = list(dataframes.keys())\nfor origin in origins:\n    train_data[origin] = 0\n    test_data[origin] = 0\n\n# Function to one-hot encode based on name substrings\ndef one_hot_encode_by_origin(df):\n    for index, row in df.iterrows():\n        name = row['Name'].lower()\n        for origin, origin_df in dataframes.items():\n            if any(subname.lower() in name for subname in origin_df['Surname']):\n                df.at[index, origin] = 1\n\n# One-hot encode the train and test data\none_hot_encode_by_origin(train_data)\none_hot_encode_by_origin(test_data)","metadata":{"_uuid":"ab8bd5ee-0999-4c75-b7c5-98c31dda4477","_cell_guid":"688809d4-6335-48a6-83d9-382e631a6ec7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:27.322532Z","iopub.execute_input":"2023-10-02T16:16:27.323218Z","iopub.status.idle":"2023-10-02T16:16:32.695944Z","shell.execute_reply.started":"2023-10-02T16:16:27.323182Z","shell.execute_reply":"2023-10-02T16:16:32.694693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"_uuid":"f6116203-ecbf-4601-b6fa-46789c55dde3","_cell_guid":"352ad4ac-d28d-4d9b-8ed3-9f1ce96304c8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:32.697917Z","iopub.execute_input":"2023-10-02T16:16:32.698783Z","iopub.status.idle":"2023-10-02T16:16:32.791809Z","shell.execute_reply.started":"2023-10-02T16:16:32.698737Z","shell.execute_reply":"2023-10-02T16:16:32.790659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Compute the sum for each country column\ntrain_sums = train_data[origins].sum()\ntest_sums = test_data[origins].sum()\n\n# 2. Concatenate the sums\nall_sums = pd.concat([train_sums, test_sums], axis=1, keys=['Train', 'Test'])\n\n# 3. Plot using matplotlib\nall_sums.plot(kind='bar', figsize=(14,7))\nplt.title('Counts of People by Country in Train and Test Data')\nplt.ylabel('Number of People')\nplt.xlabel('Country')\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"f30d75ba-2841-4ce9-a6a2-672b05eb7d14","_cell_guid":"42cb783f-9ebb-4157-aa6a-3b7bcf8b3dc6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:32.793778Z","iopub.execute_input":"2023-10-02T16:16:32.794610Z","iopub.status.idle":"2023-10-02T16:16:33.339288Z","shell.execute_reply.started":"2023-10-02T16:16:32.794567Z","shell.execute_reply":"2023-10-02T16:16:33.338091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Sum of 1s across country columns for each row\ntrain_data['sum_of_countries'] = train_data[origins].sum(axis=1)\ntest_data['sum_of_countries'] = test_data[origins].sum(axis=1)\n\n# 2. Filter rows where this sum is 2 or more\nmulti_country_df = train_data[train_data['sum_of_countries'] >= 2]\n\n# 3. Compute the percentage of such occurrences\npercent_multi_country = (multi_country_df[origins].sum() / len(multi_country_df)) * 100\n\n# 4. Plot the percentages\nplt.figure(figsize=(14,7))\npercent_multi_country.plot(kind='bar', color='c')\nplt.title('Percentage of Names Associated with Multiple Countries in Train Data')\nplt.ylabel('Percentage')\nplt.xlabel('Country')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"f481e374-44f1-45ef-9d53-fee64bf78edb","_cell_guid":"5fcba44f-e512-46ca-925e-0ec33102461b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.341444Z","iopub.execute_input":"2023-10-02T16:16:33.342302Z","iopub.status.idle":"2023-10-02T16:16:33.699354Z","shell.execute_reply.started":"2023-10-02T16:16:33.342260Z","shell.execute_reply":"2023-10-02T16:16:33.698147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We can see that the anglican version of Asian names appear too often. \n#### I have verified that Russians were, in fact, aboard the Titanic. \nLet's remove the columns representing Chinese, Japanese, Korean, and Vietnamese. Then, if somebody has all 0s on country of origin we can add them to a column called 'othercountry.'","metadata":{"_uuid":"9d528548-00ba-45b6-bec8-cef0265136ef","_cell_guid":"26220422-ac56-4f2a-86c4-80f84d98eed1","trusted":true}},{"cell_type":"code","source":"columns_to_drop = ['chinese', 'japanese', 'korean', 'vietnamese']\n\n# Drop the columns from train_data\ntrain_data = train_data.drop(columns=columns_to_drop, axis=1)\n\n# Drop the columns from test_data\ntest_data = test_data.drop(columns=columns_to_drop, axis=1)","metadata":{"_uuid":"132bef4a-99ca-4f03-b6db-984bb285ce17","_cell_guid":"516fe6ab-61cd-4ae1-8fc8-9cc42452055c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.700618Z","iopub.execute_input":"2023-10-02T16:16:33.701557Z","iopub.status.idle":"2023-10-02T16:16:33.709042Z","shell.execute_reply.started":"2023-10-02T16:16:33.701523Z","shell.execute_reply":"2023-10-02T16:16:33.707766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_check = ['arabic', 'czech', 'dutch', 'english', 'french', 'german', 'greek', 'irish', 'italian', 'polish', 'portuguese', 'scottish', 'spanish']\n\n# Check and set 'othercountry' for train_data\ntrain_data['othercountry'] = np.where(train_data[columns_to_check].sum(axis=1) == 0, 1, 0)\n\n# Check and set 'othercountry' for test_data\ntest_data['othercountry'] = np.where(test_data[columns_to_check].sum(axis=1) == 0, 1, 0)","metadata":{"_uuid":"708310a9-8b09-4a79-9a27-9f930c9ad21c","_cell_guid":"e634ab66-dcc5-43bd-95e3-101d48d3ff24","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.711368Z","iopub.execute_input":"2023-10-02T16:16:33.712343Z","iopub.status.idle":"2023-10-02T16:16:33.733937Z","shell.execute_reply.started":"2023-10-02T16:16:33.712291Z","shell.execute_reply":"2023-10-02T16:16:33.732391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Onward. Let's look to see if 'Cabin' shows a pattern","metadata":{"_uuid":"6ec22c98-b165-4e89-97cc-932ce2972a84","_cell_guid":"650db0d3-af23-4c0c-b44a-42d19c88061a","trusted":true}},{"cell_type":"code","source":"test_data['Cabin'].unique()","metadata":{"_uuid":"c13d1e07-ccca-479e-b4be-47db14d51c32","_cell_guid":"9353b3d1-208f-4495-9af7-b9aea22a440a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.735116Z","iopub.execute_input":"2023-10-02T16:16:33.735956Z","iopub.status.idle":"2023-10-02T16:16:33.750778Z","shell.execute_reply.started":"2023-10-02T16:16:33.735923Z","shell.execute_reply":"2023-10-02T16:16:33.749272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## There is a pattern. It starts with A-F! \n#### A quick Google Search shows this is the Sun Deck, Upper Promenade, Upper Deck, Saloon Deck, Main Deck, and Middle Deck. \n## Let's break one-hot encode this into decks","metadata":{"_uuid":"1457b593-fcad-41c1-971f-820153900f68","_cell_guid":"4e0a5309-c3ea-4f0c-8267-df24923ce129","trusted":true}},{"cell_type":"code","source":"def categorize_cabin(cabin):\n    if cabin.startswith('S'):\n        return 'Sun_Deck'\n    elif cabin.startswith('A'):\n        return 'Upper_Prom_Deck'\n    elif cabin.startswith('B'):\n        return 'Prom_Deck_Glass'\n    elif cabin.startswith('C'):\n        return 'Upper_Deck'\n    elif cabin.startswith('D'):\n        return 'Saloon_Deck'\n    elif cabin.startswith('E'):\n        return 'Main_Deck'\n    elif cabin.startswith('F'):\n        return 'Middle_Deck'\n    else:\n        return 'Unknown'\n\n# Apply the categorization function to the 'Cabin' column in both train_data and test_data\ntrain_data['Cabin_Category'] = train_data['Cabin'].apply(categorize_cabin)\ntest_data['Cabin_Category'] = test_data['Cabin'].apply(categorize_cabin)\n\n# Perform one-hot encoding on the 'Cabin_Category' column\ntrain_data = pd.get_dummies(train_data, columns=['Cabin_Category'], prefix='Cabin')\ntest_data = pd.get_dummies(test_data, columns=['Cabin_Category'], prefix='Cabin')\n\n# Drop the original 'Cabin' column\ntrain_data.drop(columns=['Cabin'], inplace=True)\ntest_data.drop(columns=['Cabin'], inplace=True)\n\n# Print the updated data\nprint(train_data.head())\nprint(test_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:33.752563Z","iopub.execute_input":"2023-10-02T16:16:33.752891Z","iopub.status.idle":"2023-10-02T16:16:33.790668Z","shell.execute_reply.started":"2023-10-02T16:16:33.752864Z","shell.execute_reply":"2023-10-02T16:16:33.789616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_values_dict = {}\n\n# Exclude 'PassengerID' column\ncolumns_to_check = [col for col in test_data.columns if col != 'PassengerID']\n\n# Find the maximum number of unique values across columns\nmax_unique_values = max([len(test_data[col].dropna().unique()) for col in columns_to_check])\n\n# Populate unique_values_dict\nfor col in columns_to_check:\n    unique_values = test_data[col].dropna().unique()\n    padding = max_unique_values - len(unique_values)\n    unique_values_dict[col] = list(unique_values) + [''] * padding\n\n# Create DataFrame\nunique_df = pd.DataFrame(unique_values_dict)\n\nunique_df.head(100)","metadata":{"_uuid":"a2cbd800-73fc-4d97-be3c-6d1625855811","_cell_guid":"484a482a-6272-4414-a25b-88d9bd4f32d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.792398Z","iopub.execute_input":"2023-10-02T16:16:33.792831Z","iopub.status.idle":"2023-10-02T16:16:33.837219Z","shell.execute_reply.started":"2023-10-02T16:16:33.792792Z","shell.execute_reply":"2023-10-02T16:16:33.836066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encode Sex","metadata":{"_uuid":"58505d16-6d5b-44e8-95b7-8303c3fc8fce","_cell_guid":"e1d76170-2565-49bd-ae8c-252f239a9454","trusted":true}},{"cell_type":"code","source":"# Perform one-hot encoding for 'Sex' column in train_data\ntrain_data = pd.get_dummies(train_data, columns=['Sex'], prefix='Sex')\n\n# Perform one-hot encoding for 'Sex' column in test_data\ntest_data = pd.get_dummies(test_data, columns=['Sex'], prefix='Sex')\n\nprint(train_data.head())\nprint(test_data.head())","metadata":{"_uuid":"76cb6586-0dec-44c4-abcc-5f7d7c7944bf","_cell_guid":"73b8bd25-774c-4249-9b19-ecd1a28383e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.838503Z","iopub.execute_input":"2023-10-02T16:16:33.838815Z","iopub.status.idle":"2023-10-02T16:16:33.868065Z","shell.execute_reply.started":"2023-10-02T16:16:33.838787Z","shell.execute_reply":"2023-10-02T16:16:33.866739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One-Hot Encode Embarked","metadata":{"_uuid":"05682b4a-a726-40ca-8066-331d12afc55b","_cell_guid":"32f23943-341f-4c70-96fa-0f791cf3e8a5","trusted":true}},{"cell_type":"code","source":"# Perform one-hot encoding for 'Embarked' column in train_data\ntrain_data = pd.get_dummies(train_data, columns=['Embarked'], prefix=['Embarked'])\n\n# Perform one-hot encoding for 'Embarked' column in test_data\ntest_data = pd.get_dummies(test_data, columns=['Embarked'], prefix=['Embarked'])\n\nprint(train_data.head())\nprint(test_data.head())","metadata":{"_uuid":"fb7b5e3e-f187-45df-b617-fdadc7385ad2","_cell_guid":"924c5781-0f2d-4f1a-ac25-fdc58517f524","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.869966Z","iopub.execute_input":"2023-10-02T16:16:33.870427Z","iopub.status.idle":"2023-10-02T16:16:33.897692Z","shell.execute_reply.started":"2023-10-02T16:16:33.870385Z","shell.execute_reply":"2023-10-02T16:16:33.896569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We don't want to be redundant with 'Mr.' and 'Mrs,' so let's drop the names, as well as Ticket and Title","metadata":{"_uuid":"72a8bc34-e37a-4fd6-b206-039842e3d4f6","_cell_guid":"74090405-1306-46ba-9a95-18982b7149bb","trusted":true}},{"cell_type":"code","source":"train_data.drop(columns=['Name'], inplace=True)\ntest_data.drop(columns=['Name'], inplace=True)\ntrain_data.drop(columns=['Ticket'], inplace=True)\ntest_data.drop(columns=['Ticket'], inplace=True)\ntest_data.drop(columns=['Title'], inplace=True)\ntrain_data.drop(columns=['Title'], inplace=True)","metadata":{"_uuid":"a5e404e4-e5fb-4e25-b0ac-5cdb623e44be","_cell_guid":"066c01b5-4e0c-42e1-b8f3-df89eb0a534c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.898916Z","iopub.execute_input":"2023-10-02T16:16:33.899293Z","iopub.status.idle":"2023-10-02T16:16:33.913449Z","shell.execute_reply.started":"2023-10-02T16:16:33.899264Z","shell.execute_reply":"2023-10-02T16:16:33.912236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"_uuid":"765f4108-8388-4c43-8cad-9c78d918c2ec","_cell_guid":"4cae0ea5-9e6e-4d4b-bd0e-4442338fa86f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.914874Z","iopub.execute_input":"2023-10-02T16:16:33.915274Z","iopub.status.idle":"2023-10-02T16:16:33.932085Z","shell.execute_reply.started":"2023-10-02T16:16:33.915218Z","shell.execute_reply":"2023-10-02T16:16:33.930992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"_uuid":"29da52ee-3021-4fde-80d5-a02d34780575","_cell_guid":"145b690a-78c5-49aa-ba9f-a9e7dd57aebd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:33.933643Z","iopub.execute_input":"2023-10-02T16:16:33.934723Z","iopub.status.idle":"2023-10-02T16:16:33.949545Z","shell.execute_reply.started":"2023-10-02T16:16:33.934676Z","shell.execute_reply":"2023-10-02T16:16:33.948458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## View a histogram of the fare","metadata":{"_uuid":"f6c20bf4-0fa1-4acc-8738-b8f9202a0227","_cell_guid":"469d6bd7-48ea-48c3-ad7f-09f575850867","trusted":true}},{"cell_type":"code","source":"fig = px.histogram(train_data, x='Fare', nbins=20, title='Histogram of Fare')\nfig.update_xaxes(title_text='Fare')\nfig.update_yaxes(title_text='Frequency')\nfig.show()","metadata":{"_uuid":"6d7d5769-cd31-4e0b-aeab-a917deb63511","_cell_guid":"3ac0b154-42f4-488f-aa0c-feae315618f4","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:33.950880Z","iopub.execute_input":"2023-10-02T16:16:33.951378Z","iopub.status.idle":"2023-10-02T16:16:34.314610Z","shell.execute_reply.started":"2023-10-02T16:16:33.951348Z","shell.execute_reply":"2023-10-02T16:16:34.313533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### There seems to be quite the outlier(s). Let's explore further","metadata":{"_uuid":"3cdb9624-cdc3-4889-81cb-db38f5bbf470","_cell_guid":"413f7826-f343-47d2-8fcf-31846d79e462","trusted":true}},{"cell_type":"code","source":"fig = px.box(train_data, y='Fare', title='Box Plot of Fare')\nfig.update_xaxes(title_text='Fare')\nfig.update_yaxes(title_text='Value')\nfig.show()","metadata":{"_uuid":"918ca89d-9f35-4d34-a81f-b54936309793","_cell_guid":"854ce2fd-8325-4431-9842-eba68e06bf0b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:34.315759Z","iopub.execute_input":"2023-10-02T16:16:34.316678Z","iopub.status.idle":"2023-10-02T16:16:34.382635Z","shell.execute_reply.started":"2023-10-02T16:16:34.316645Z","shell.execute_reply":"2023-10-02T16:16:34.381343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\nsns.kdeplot(train_data['Fare'], label='train_data', fill=True)\nsns.kdeplot(test_data['Fare'], label='test_data', fill=True)\n\nplt.title('Fare in train_data and test_data')\nplt.xlabel('Fare')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"d0212871-d2f2-4e53-aca9-99b8ea0b3879","_cell_guid":"567c0e0d-ed1b-45fa-832e-62c885ad64d5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:34.384207Z","iopub.execute_input":"2023-10-02T16:16:34.384533Z","iopub.status.idle":"2023-10-02T16:16:35.010426Z","shell.execute_reply.started":"2023-10-02T16:16:34.384508Z","shell.execute_reply":"2023-10-02T16:16:35.008649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fare is skewed. \n#### Run a log function to normalize","metadata":{"_uuid":"fa8c5381-15ab-49c1-bf3d-a5b884e73bab","_cell_guid":"88e99932-78d5-4ea7-afc3-c3bce53de1e0","trusted":true}},{"cell_type":"code","source":"# Apply log transformation to 'Fare' in train_data and test_data\ntrain_data['Fare'] = np.log1p(train_data['Fare'])\ntest_data['Fare'] = np.log1p(test_data['Fare'])\n\n# Now plot the KDE again\nplt.figure(figsize=(12, 6))\n\nsns.kdeplot(train_data['Fare'], label='train_data', fill=True)\nsns.kdeplot(test_data['Fare'], label='test_data', fill=True)\n\nplt.title('Log-Transformed Fare in train_data and test_data')\nplt.xlabel('Log-Transformed Fare')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"906da65e-23b2-4e7f-9876-3bd0f9da6e92","_cell_guid":"b0b58c83-c038-49ac-b397-ebc97ddb95c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:35.012088Z","iopub.execute_input":"2023-10-02T16:16:35.012571Z","iopub.status.idle":"2023-10-02T16:16:35.300260Z","shell.execute_reply.started":"2023-10-02T16:16:35.012541Z","shell.execute_reply":"2023-10-02T16:16:35.299121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Verify","metadata":{"_uuid":"06c7c165-2fbb-4620-94fc-6879a2989509","_cell_guid":"d2fbd2d1-e49c-4267-95a5-2efeec70832d","trusted":true}},{"cell_type":"code","source":"fig = px.box(train_data, y='Fare', title='Box Plot of Fare')\nfig.update_xaxes(title_text='Fare')\nfig.update_yaxes(title_text='Value')\nfig.show()","metadata":{"_uuid":"b57bd194-2dde-47dc-a783-67015126feba","_cell_guid":"47aebdbb-87f3-4db3-8e60-1cb31c9f8cd7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:35.301531Z","iopub.execute_input":"2023-10-02T16:16:35.301845Z","iopub.status.idle":"2023-10-02T16:16:35.361262Z","shell.execute_reply.started":"2023-10-02T16:16:35.301818Z","shell.execute_reply":"2023-10-02T16:16:35.360212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"much better!","metadata":{"_uuid":"321bfabf-e391-494e-ba32-95bfacb5d57a","_cell_guid":"0fb50f7d-46e6-473c-80b9-7a64d0755130","trusted":true}},{"cell_type":"code","source":"test_data.head()","metadata":{"_uuid":"4701ad39-0df2-4f7f-93cf-b6b54a1c0cf7","_cell_guid":"aec4f085-034d-46fb-8539-c71e5f112e28","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:35.362403Z","iopub.execute_input":"2023-10-02T16:16:35.362723Z","iopub.status.idle":"2023-10-02T16:16:35.383384Z","shell.execute_reply.started":"2023-10-02T16:16:35.362696Z","shell.execute_reply":"2023-10-02T16:16:35.382599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"_uuid":"135a282f-a984-4214-b436-22142ac6dc40","_cell_guid":"e06f6197-4811-4547-868d-5446ce98c70d","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:35.384284Z","iopub.execute_input":"2023-10-02T16:16:35.384583Z","iopub.status.idle":"2023-10-02T16:16:35.402627Z","shell.execute_reply.started":"2023-10-02T16:16:35.384556Z","shell.execute_reply":"2023-10-02T16:16:35.401807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"_uuid":"cc8bac5e-621a-47fc-812f-14b0a8278417","_cell_guid":"f0e62493-4fa1-4b25-88b8-824a1db5c13d","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:35.403992Z","iopub.execute_input":"2023-10-02T16:16:35.404323Z","iopub.status.idle":"2023-10-02T16:16:35.422086Z","shell.execute_reply.started":"2023-10-02T16:16:35.404297Z","shell.execute_reply":"2023-10-02T16:16:35.420929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"_uuid":"5fa68e54-c7ae-45e9-afb8-00fa9c4b6b59","_cell_guid":"5f9e102c-6c27-414a-867b-69a398e78918","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:35.423273Z","iopub.execute_input":"2023-10-02T16:16:35.423646Z","iopub.status.idle":"2023-10-02T16:16:35.437856Z","shell.execute_reply.started":"2023-10-02T16:16:35.423616Z","shell.execute_reply":"2023-10-02T16:16:35.436443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"_uuid":"e81c3ec4-d40d-4e41-a5dc-ebf412b29b03","_cell_guid":"89179fbc-aec0-42f1-ae74-3665ff809943","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:35.450165Z","iopub.execute_input":"2023-10-02T16:16:35.450996Z","iopub.status.idle":"2023-10-02T16:16:35.470034Z","shell.execute_reply.started":"2023-10-02T16:16:35.450962Z","shell.execute_reply":"2023-10-02T16:16:35.468982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 'Age' Still has some null values. \n#### Let's use predictive modeling to guess their values.","metadata":{"_uuid":"40aa0b91-2956-4941-88ed-d79e0e8f5f82","_cell_guid":"cf49f677-8e54-43a5-ad32-3aa101d2af78","trusted":true}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer","metadata":{"_uuid":"6d88da5c-f134-464b-b7c5-f65080035928","_cell_guid":"34c49d0a-f795-4e5d-b822-9f01d0fdde51","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:35.471336Z","iopub.execute_input":"2023-10-02T16:16:35.471631Z","iopub.status.idle":"2023-10-02T16:16:35.476981Z","shell.execute_reply.started":"2023-10-02T16:16:35.471606Z","shell.execute_reply":"2023-10-02T16:16:35.475499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['Cabin_Main_Deck', 'Cabin_Middle_Deck', 'Cabin_Prom_Deck_Glass', 'Cabin_Saloon_Deck', 'Cabin_Unknown', 'Cabin_Upper_Deck', 'Cabin_Upper_Prom_Deck', 'Mr.', 'Mrs.', 'Miss.', 'Master.', 'Other', \"Sex_male\", \"Sex_female\", \"SibSp\", \"Parch\", \"Embarked_C\", \"Embarked_Q\", \"Embarked_S\", \"Fare\",\"arabic\", \"czech\", \"dutch\", \"english\", \"french\", \"german\", \"greek\", \"irish\", \"italian\", \"polish\", \"portuguese\", \"scottish\", \"spanish\", \"othercountry\",'Age']","metadata":{"_uuid":"d8cf7646-1327-467b-b6fd-235e46013b09","_cell_guid":"77c0a7d8-3b75-43f7-b0be-1dec00e61175","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:35.479256Z","iopub.execute_input":"2023-10-02T16:16:35.479583Z","iopub.status.idle":"2023-10-02T16:16:35.489871Z","shell.execute_reply.started":"2023-10-02T16:16:35.479555Z","shell.execute_reply":"2023-10-02T16:16:35.488548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify Couples","metadata":{}},{"cell_type":"code","source":"# For train_data\ntrain_data['YoungCoupleTrip'] = 0\ntrain_data.loc[(train_data['SibSp'] == 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 17) & (train_data['Age'] < 24), 'YoungCoupleTrip'] = 1\n\n# For test_data\ntest_data['YoungCoupleTrip'] = 0\ntest_data.loc[(test_data['SibSp'] == 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 17) & (test_data['Age'] < 24), 'YoungCoupleTrip'] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.491405Z","iopub.execute_input":"2023-10-02T16:16:35.492330Z","iopub.status.idle":"2023-10-02T16:16:35.509432Z","shell.execute_reply.started":"2023-10-02T16:16:35.492284Z","shell.execute_reply":"2023-10-02T16:16:35.507578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For YoungishCouples\ntrain_data['YoungishCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] == 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 23) & (train_data['Age'] < 35), 'YoungishCouples'] = 1\n\ntest_data['YoungishCouples'] = 0\ntest_data.loc[(test_data['SibSp'] == 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 23) & (test_data['Age'] < 35), 'YoungishCouples'] = 1\n\n# For MidAgeCouples\ntrain_data['MidAgeCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] == 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 35) & (train_data['Age'] < 50), 'MidAgeCouples'] = 1\n\ntest_data['MidAgeCouples'] = 0\ntest_data.loc[(test_data['SibSp'] == 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 35) & (test_data['Age'] < 50), 'MidAgeCouples'] = 1\n\n# For GrandCouples\ntrain_data['GrandCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] == 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 49), 'GrandCouples'] = 1\n\ntest_data['GrandCouples'] = 0\ntest_data.loc[(test_data['SibSp'] == 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 49), 'GrandCouples'] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.511010Z","iopub.execute_input":"2023-10-02T16:16:35.511404Z","iopub.status.idle":"2023-10-02T16:16:35.535627Z","shell.execute_reply.started":"2023-10-02T16:16:35.511362Z","shell.execute_reply":"2023-10-02T16:16:35.534374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify Young Families ","metadata":{}},{"cell_type":"code","source":"# For train_data\ntrain_data['YoungParents'] = 0\ntrain_data.loc[(train_data['SibSp'] >= 1) & (train_data['Parch'] == 1) & (train_data['Age'] > 17) & (train_data['Age'] < 24), 'YoungParents'] = 1\n\n# For test_data\ntest_data['YoungParents'] = 0\ntest_data.loc[(test_data['SibSp'] >= 1) & (test_data['Parch'] == 1) & (test_data['Age'] > 17) & (test_data['Age'] < 24), 'YoungParents'] = 1\n# For YoungishCouples\ntrain_data['YoungishCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] >= 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 23) & (train_data['Age'] < 35), 'YoungishParents'] = 1\n\ntest_data['YoungishCouples'] = 0\ntest_data.loc[(test_data['SibSp'] >= 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 23) & (test_data['Age'] < 35), 'YoungishParents'] = 1\n\n# For MidAgeCouples\ntrain_data['MidAgeCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] >= 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 35) & (train_data['Age'] < 50), 'MidAgeParents'] = 1\n\ntest_data['MidAgeCouples'] = 0\ntest_data.loc[(test_data['SibSp'] >= 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 35) & (test_data['Age'] < 50), 'MidAgeParents'] = 1\n\n# For GrandCouples\ntrain_data['GrandCouples'] = 0\ntrain_data.loc[(train_data['SibSp'] >= 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 49), 'GrandParents'] = 1\n\ntest_data['GrandCouples'] = 0\ntest_data.loc[(test_data['SibSp'] >= 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 49), 'GrandParents'] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.537172Z","iopub.execute_input":"2023-10-02T16:16:35.537941Z","iopub.status.idle":"2023-10-02T16:16:35.570636Z","shell.execute_reply.started":"2023-10-02T16:16:35.537898Z","shell.execute_reply":"2023-10-02T16:16:35.569627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode Family Size","metadata":{}},{"cell_type":"code","source":"# Create a FamilySize column\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n\n# Create an Alone column, 1 if FamilySize is 1 else 0\ntrain_data['Alone'] = (train_data['FamilySize'] == 1).astype(int)\ntest_data['Alone'] = (test_data['FamilySize'] == 1).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.571859Z","iopub.execute_input":"2023-10-02T16:16:35.572778Z","iopub.status.idle":"2023-10-02T16:16:35.582496Z","shell.execute_reply.started":"2023-10-02T16:16:35.572747Z","shell.execute_reply":"2023-10-02T16:16:35.581335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Group by FamilySize to get counts\nfamily_size_counts = train_data['FamilySize'].value_counts().sort_index()\n\n# Create a bar plot\nfig = px.bar(family_size_counts, \n             x=family_size_counts.index, \n             y=family_size_counts.values, \n             labels={'x':'Family Size', 'y':'Number of Passengers'},\n             title=\"Distribution of FamilySize in train_data\")\n\n# Show plot\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.583963Z","iopub.execute_input":"2023-10-02T16:16:35.584297Z","iopub.status.idle":"2023-10-02T16:16:35.652247Z","shell.execute_reply.started":"2023-10-02T16:16:35.584269Z","shell.execute_reply":"2023-10-02T16:16:35.651473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode Family Size\nWe already have a column titled \"Alone.\" Now, let's use the chart to one hot encode the category 2, 3, 4 and 5 together, 6 plus together. ","metadata":{}},{"cell_type":"code","source":"# One-hot encode FamilySize for the category of size 2\ntrain_data['TwoPass'] = train_data['FamilySize'].apply(lambda x: 1 if x == 2 else 0)\ntest_data['TwoPass'] = test_data['FamilySize'].apply(lambda x: 1 if x == 2 else 0)\n\n# One-hot encode FamilySize for the category of size 3\ntrain_data['ThreePass'] = train_data['FamilySize'].apply(lambda x: 1 if x == 3 else 0)\ntest_data['ThreePass'] = test_data['FamilySize'].apply(lambda x: 1 if x == 3 else 0)\n\n# One-hot encode FamilySize for the combined category of sizes 4 and 5\ntrain_data['FourFivePass'] = train_data['FamilySize'].apply(lambda x: 1 if x == 4 or x == 5 else 0)\ntest_data['FourFivePass'] = test_data['FamilySize'].apply(lambda x: 1 if x == 4 or x == 5 else 0)\n\n# One-hot encode FamilySize for the category of size 6 and above\ntrain_data['SixPlusPass'] = train_data['FamilySize'].apply(lambda x: 1 if x >= 6 else 0)\ntest_data['SixPlusPass'] = test_data['FamilySize'].apply(lambda x: 1 if x >= 6 else 0)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.653727Z","iopub.execute_input":"2023-10-02T16:16:35.654048Z","iopub.status.idle":"2023-10-02T16:16:35.673756Z","shell.execute_reply.started":"2023-10-02T16:16:35.654021Z","shell.execute_reply":"2023-10-02T16:16:35.672731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Identify Likely Young Couples","metadata":{}},{"cell_type":"code","source":"# For train_data\ntrain_data['YoungCoupleTrip'] = 0\ntrain_data.loc[(train_data['SibSp'] == 1) & (train_data['Parch'] == 0) & (train_data['Age'] > 17) & (train_data['Age'] < 24), 'YoungCoupleTrip'] = 1\n\n# For test_data\ntest_data['YoungCoupleTrip'] = 0\ntest_data.loc[(test_data['SibSp'] == 1) & (test_data['Parch'] == 0) & (test_data['Age'] > 17) & (test_data['Age'] < 24), 'YoungCoupleTrip'] = 1\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.675436Z","iopub.execute_input":"2023-10-02T16:16:35.675762Z","iopub.status.idle":"2023-10-02T16:16:35.686160Z","shell.execute_reply.started":"2023-10-02T16:16:35.675733Z","shell.execute_reply":"2023-10-02T16:16:35.685196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop('FamilySize', axis=1, inplace=True)\ntest_data.drop('FamilySize', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.687607Z","iopub.execute_input":"2023-10-02T16:16:35.687933Z","iopub.status.idle":"2023-10-02T16:16:35.703139Z","shell.execute_reply.started":"2023-10-02T16:16:35.687907Z","shell.execute_reply":"2023-10-02T16:16:35.701736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One Hot Encode Fare into sixths","metadata":{}},{"cell_type":"code","source":"# Determine the Fare range\nmin_fare = train_data['Fare'].min()\nmax_fare = train_data['Fare'].max()\n\n# Calculate the interval length\ninterval_length = (max_fare - min_fare) / 6\n\n# Assign each Fare to its respective interval\ndef assign_interval(fare):\n    for i in range(6):\n        if fare <= min_fare + interval_length * (i+1):\n            return f\"Fare_{i+1}\"\n    return f\"Fare_6\"\n\ntrain_data['Fare_Interval'] = train_data['Fare'].apply(assign_interval)\ntest_data['Fare_Interval'] = test_data['Fare'].apply(assign_interval)\n\n# One-hot encode the intervals\ntrain_data = pd.get_dummies(train_data, columns=['Fare_Interval'], drop_first=False)\ntest_data = pd.get_dummies(test_data, columns=['Fare_Interval'], drop_first=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.704515Z","iopub.execute_input":"2023-10-02T16:16:35.704897Z","iopub.status.idle":"2023-10-02T16:16:35.730029Z","shell.execute_reply.started":"2023-10-02T16:16:35.704811Z","shell.execute_reply":"2023-10-02T16:16:35.728728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Drop Fare","metadata":{}},{"cell_type":"code","source":"train_data.drop('Fare', axis=1, inplace=True)\ntest_data.drop('Fare', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.731962Z","iopub.execute_input":"2023-10-02T16:16:35.732640Z","iopub.status.idle":"2023-10-02T16:16:35.740897Z","shell.execute_reply.started":"2023-10-02T16:16:35.732595Z","shell.execute_reply":"2023-10-02T16:16:35.739092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Verify and Inspect all Features","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.742891Z","iopub.execute_input":"2023-10-02T16:16:35.743390Z","iopub.status.idle":"2023-10-02T16:16:35.768167Z","shell.execute_reply.started":"2023-10-02T16:16:35.743349Z","shell.execute_reply":"2023-10-02T16:16:35.766991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Use this to define 'features'","metadata":{}},{"cell_type":"code","source":"features = [\n    \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \n    \"MsMlle\", \"MrsMme\", \"Nobility\", \"ProTitle\", \n    \"Military\", \"Mr.\", \"Master.\", \n    \"arabic\", \"czech\", \"dutch\", \"english\", \n    \"french\", \"german\", \"greek\", \"irish\", \n    \"italian\", \"polish\", \"portuguese\", \n    \"russian\", \"scottish\", \"spanish\", \n    \"sum_of_countries\", \"othercountry\", \n    \"Cabin_Main_Deck\", \"Cabin_Middle_Deck\", \n    \"Cabin_Prom_Deck_Glass\", \"Cabin_Saloon_Deck\", \n    \"Cabin_Unknown\", \"Cabin_Upper_Deck\", \n    \"Cabin_Upper_Prom_Deck\", \"Sex_female\", \n    \"Sex_male\", \"Embarked_C\", \"Embarked_Q\", \n    \"Embarked_S\", \"YoungCoupleTrip\", \n    \"YoungishCouples\", \"MidAgeCouples\", \n    \"GrandCouples\", \"YoungParents\", \n    \"YoungishParents\", \"MidAgeParents\", \n    \"GrandParents\", \"Alone\", \"TwoPass\", \n    \"ThreePass\", \"FourFivePass\", \"SixPlusPass\", \n    \"Fare_Interval_Fare_1\", \"Fare_Interval_Fare_2\", \n    \"Fare_Interval_Fare_3\", \"Fare_Interval_Fare_4\", \n    \"Fare_Interval_Fare_5\", \"Fare_Interval_Fare_6\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:35.769365Z","iopub.execute_input":"2023-10-02T16:16:35.769678Z","iopub.status.idle":"2023-10-02T16:16:35.776760Z","shell.execute_reply.started":"2023-10-02T16:16:35.769650Z","shell.execute_reply":"2023-10-02T16:16:35.775618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract columns needed for imputation\ntrain_subset = train_data[features]\ntest_subset = test_data[features]\n\n# Instantiate and fit the imputer\nimputer = IterativeImputer(max_iter=10, random_state=42)\nimputer.fit(train_subset)\n\n# Apply imputation on train and test data\ntrain_data_imputed = imputer.transform(train_subset)\ntest_data_imputed = imputer.transform(test_subset)\n\n# Convert imputed data back to DataFrame and update original data\ntrain_data[features] = pd.DataFrame(train_data_imputed, columns=features)\ntest_data[features] = pd.DataFrame(test_data_imputed, columns=features)","metadata":{"_uuid":"c416cd7a-a0fa-4704-b31b-d4ca5c182f8b","_cell_guid":"c78fc735-4530-437a-b74b-ea18adfa68af","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:35.778400Z","iopub.execute_input":"2023-10-02T16:16:35.778832Z","iopub.status.idle":"2023-10-02T16:16:37.106208Z","shell.execute_reply.started":"2023-10-02T16:16:35.778794Z","shell.execute_reply":"2023-10-02T16:16:37.104940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_df = pd.DataFrame(test_data.isnull().sum())\nprint(test_data_df)\ntrain_data_df = pd.DataFrame(train_data.isnull().sum())\nprint(train_data_df)","metadata":{"_uuid":"a0e2682d-d3cf-4b8b-9026-4d1075b6f975","_cell_guid":"2da1ce2f-3546-4ecc-acf4-90c26be3066f","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:37.107668Z","iopub.execute_input":"2023-10-02T16:16:37.108436Z","iopub.status.idle":"2023-10-02T16:16:37.151414Z","shell.execute_reply.started":"2023-10-02T16:16:37.108389Z","shell.execute_reply":"2023-10-02T16:16:37.150085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add a new column \"AgeInt\" by converting the \"Age\" column to integers\ntrain_data['AgeInt'] = train_data['Age'].astype(int)\n\n# Calculate the count of each unique age\nage_counts = train_data['AgeInt'].value_counts().reset_index()\nage_counts.columns = ['AgeInt', 'Count']\n\n# Plot using sns.barplot\nplt.figure(figsize=(10, 6))\nsns.barplot(data=age_counts, x='AgeInt', y='Count', palette='viridis')\nplt.title('Count of Each Age in train_data')\nplt.ylabel('Count')\nplt.xlabel('Age')\nplt.xticks(rotation=45)\nplt.show()\n\n# Calculate the percentage of 'Transported' for each age group using \"AgeInt\"\nage_transported_percentage = train_data.groupby('AgeInt')['Survived'].mean() * 100\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))\nsns.barplot(x=age_transported_percentage.index, y=age_transported_percentage.values, palette='viridis')\nplt.xlabel('Age')\nplt.ylabel('Percentage Transported')\nplt.title('Percentage of Transported by Age')\nplt.xticks(rotation=45)\nplt.show()\n\n# Remove the \"AgeInt\" column\ntrain_data.drop('AgeInt', axis=1, inplace=True)","metadata":{"_uuid":"bada69f8-b7c1-46c5-97a2-bb3b3036e33a","_cell_guid":"b28f15f5-a93c-428a-8125-9b2d205ff7b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:37.153678Z","iopub.execute_input":"2023-10-02T16:16:37.154813Z","iopub.status.idle":"2023-10-02T16:16:38.591786Z","shell.execute_reply.started":"2023-10-02T16:16:37.154732Z","shell.execute_reply":"2023-10-02T16:16:38.590728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### bin by threes and view again","metadata":{"_uuid":"309cc25c-0e2e-4521-8124-84484babaa46","_cell_guid":"918e255c-c4ac-4a48-9d23-b3e0c0b46131","trusted":true}},{"cell_type":"code","source":"# Bin the ages by threes\nmax_age = train_data['Age'].max()\nbins = list(range(0, int(max_age) + 4, 3))\nlabels = [f'{i}-{i+2}' for i in bins[:-1]]\n\ntrain_data['Age_bins'] = pd.cut(train_data['Age'], bins=bins, labels=labels, right=False)\ntest_data['Age_bins'] = pd.cut(test_data['Age'], bins=bins, labels=labels, right=False)\n\n# Calculate counts for each bin\nage_bin_counts = train_data['Age_bins'].value_counts().sort_index().reset_index()\nage_bin_counts.columns = ['Age Bins', 'Count']\n\n# Plot using sns.barplot\nplt.figure(figsize=(15, 6))\nsns.barplot(data=age_bin_counts, x='Age Bins', y='Count', palette='viridis')\nplt.title('Distribution of Age in 3-Year Bins')\nplt.ylabel('Count')\nplt.xlabel('Age Bins')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"_uuid":"0c6e7755-803e-4f5f-a188-a2a1ee10ae4e","_cell_guid":"72bf9abf-077d-4912-8024-96f58a30f816","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:38.593482Z","iopub.execute_input":"2023-10-02T16:16:38.594049Z","iopub.status.idle":"2023-10-02T16:16:39.015555Z","shell.execute_reply.started":"2023-10-02T16:16:38.594019Z","shell.execute_reply":"2023-10-02T16:16:39.014533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## One hot encode these age bins and drop the age and age bin columns","metadata":{"_uuid":"373fbc8f-eeb1-46a9-b0e8-a6411511817e","_cell_guid":"8a7e8034-6d43-4891-b8a2-f0b41b2f92f3","trusted":true}},{"cell_type":"code","source":"# One-hot encode the Age_bins column\nage_dummies = pd.get_dummies(train_data['Age_bins'], prefix='AgeBin')\n\n# Concatenate the one-hot encoded columns to the original DataFrame\ntrain_data = pd.concat([train_data, age_dummies], axis=1)\n\n# Drop the Age and Age_bins columns\ntrain_data.drop(['Age', 'Age_bins'], axis=1, inplace=True)\n\n# One-hot encode the Age_bins column\nage_dummies = pd.get_dummies(test_data['Age_bins'], prefix='AgeBin')\n\n# Concatenate the one-hot encoded columns to the original DataFrame\ntest_data = pd.concat([test_data, age_dummies], axis=1)\n\n# Drop the Age and Age_bins columns\ntest_data.drop(['Age', 'Age_bins'], axis=1, inplace=True)","metadata":{"_uuid":"4f58293f-d751-42a9-b509-3108f879195e","_cell_guid":"d3c6cbb7-6ef1-4143-8fa2-722513737efc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:39.016918Z","iopub.execute_input":"2023-10-02T16:16:39.017800Z","iopub.status.idle":"2023-10-02T16:16:39.034716Z","shell.execute_reply.started":"2023-10-02T16:16:39.017770Z","shell.execute_reply":"2023-10-02T16:16:39.033651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features.remove('Age')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.036624Z","iopub.execute_input":"2023-10-02T16:16:39.037138Z","iopub.status.idle":"2023-10-02T16:16:39.042911Z","shell.execute_reply.started":"2023-10-02T16:16:39.037080Z","shell.execute_reply":"2023-10-02T16:16:39.041336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of AgeBin features\nagebin_features = [\n    'AgeBin_0-2', 'AgeBin_3-5', 'AgeBin_6-8', 'AgeBin_9-11', 'AgeBin_12-14', 'AgeBin_15-17',\n    'AgeBin_18-20', 'AgeBin_21-23', 'AgeBin_24-26', 'AgeBin_27-29', 'AgeBin_30-32', \n    'AgeBin_33-35', 'AgeBin_36-38', 'AgeBin_39-41', 'AgeBin_42-44', 'AgeBin_45-47',\n    'AgeBin_48-50', 'AgeBin_51-53', 'AgeBin_54-56', 'AgeBin_57-59', 'AgeBin_60-62', \n    'AgeBin_63-65', 'AgeBin_66-68', 'AgeBin_69-71', 'AgeBin_72-74', 'AgeBin_75-77', \n    'AgeBin_78-80'\n]\n\n# Extend the original features list with the AgeBin features\nfeatures.extend(agebin_features)","metadata":{"_uuid":"f76a1ee5-f8a8-494e-a496-d26001c66020","_cell_guid":"f58e50dd-276f-4147-8bd4-6e4173c7a6b2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:39.044965Z","iopub.execute_input":"2023-10-02T16:16:39.045703Z","iopub.status.idle":"2023-10-02T16:16:39.057432Z","shell.execute_reply.started":"2023-10-02T16:16:39.045656Z","shell.execute_reply":"2023-10-02T16:16:39.055833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### View all features","metadata":{}},{"cell_type":"code","source":"features","metadata":{"_uuid":"4b34ffbb-69ac-4302-8c6d-49dceeb65b20","_cell_guid":"e226d78e-cc2a-45d8-90bc-64d9584a6c1a","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:39.058889Z","iopub.execute_input":"2023-10-02T16:16:39.059893Z","iopub.status.idle":"2023-10-02T16:16:39.072455Z","shell.execute_reply.started":"2023-10-02T16:16:39.059843Z","shell.execute_reply":"2023-10-02T16:16:39.071433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Selection\n\n### We're predicting \"Survived,\" which is a classification prediction","metadata":{"_uuid":"935f7e06-395a-497a-8281-521337f71852","_cell_guid":"e986925b-d642-4258-b2ff-8281cdc6049c","trusted":true}},{"cell_type":"code","source":"train_data.shape","metadata":{"_uuid":"54627e4e-87a2-4b0b-99a2-e045828e1267","_cell_guid":"62ff6c78-2ee8-4115-80c2-37ba1bb0ba40","collapsed":false,"jupyter":{"outputs_hidden":false},"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:39.073807Z","iopub.execute_input":"2023-10-02T16:16:39.074480Z","iopub.status.idle":"2023-10-02T16:16:39.092404Z","shell.execute_reply.started":"2023-10-02T16:16:39.074450Z","shell.execute_reply":"2023-10-02T16:16:39.091206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the display options to show more rows\npd.set_option('display.max_rows', None)\n\n# Create a DataFrame with columns of train_data and test_data for comparison\ncomparison_df = pd.DataFrame({\n    \"train_data columns\": pd.Series(train_data.columns),\n    \"test_data columns\": pd.Series(test_data.columns)\n})\n\n# Display the comparison DataFrame\nprint(comparison_df)\n\n# Reset the display option to the default, if desired\npd.reset_option('display.max_rows')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-10-02T16:16:39.094086Z","iopub.execute_input":"2023-10-02T16:16:39.094474Z","iopub.status.idle":"2023-10-02T16:16:39.110606Z","shell.execute_reply.started":"2023-10-02T16:16:39.094444Z","shell.execute_reply":"2023-10-02T16:16:39.109215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare XGBoost & Fine Tune\nThis ran in Anaconda and doesn't need to run again. Keep scrolling to see best parameters","metadata":{"_uuid":"cc917d15-1284-4bf6-a74f-857ff99c62c6","_cell_guid":"df977430-1f39-46f0-93ec-f6a50b632399","trusted":true}},{"cell_type":"code","source":"# Separate X and y from train_data using 'features'\nX_train = train_data[features]\ny_train = train_data['Survived']\n\n# Split train_data into training and validation sets\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=64)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.112378Z","iopub.execute_input":"2023-10-02T16:16:39.112789Z","iopub.status.idle":"2023-10-02T16:16:39.124917Z","shell.execute_reply.started":"2023-10-02T16:16:39.112760Z","shell.execute_reply":"2023-10-02T16:16:39.123917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Initialize XGBoost model\n# xgb_model = XGBClassifier(\n#     objective='binary:logistic',\n#     n_estimators=500,\n#     max_depth=10,\n#     learning_rate=0.01,\n#     subsample=1,\n#     colsample_bytree=0.6,\n#     reg_lambda=0.1,\n#     reg_alpha=0.1,\n#     random_state=64\n# )","metadata":{"_uuid":"b3dd1360-800a-491d-b47b-66e4fd25b5ea","_cell_guid":"93a528b1-9065-40d8-88ac-344728c8e6bb","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-02T16:16:39.126924Z","iopub.execute_input":"2023-10-02T16:16:39.127909Z","iopub.status.idle":"2023-10-02T16:16:39.133224Z","shell.execute_reply.started":"2023-10-02T16:16:39.127863Z","shell.execute_reply":"2023-10-02T16:16:39.132044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Update hyperparameter search space for XGBoost\n# param_grid = {\n#     'n_estimators': [100, 500, 1000],\n#     'max_depth': [2, 3, 4, 5, 6, 7],\n#     'learning_rate': [0.001, 0.005, 0.01],\n#     'subsample': [0.5,0.6, 0.7, 0.8, 0.9, 1],\n#     'colsample_bytree': [0.7, 0.75, 0.8, 0.85, 9],\n#     'reg_lambda': np.logspace(-3, 0, 2),\n#     'reg_alpha': np.logspace(-3, 0, 2)\n# }\n\n\n# # Use GridSearch\n# search = GridSearchCV(xgb_model, param_grid=param_grid, scoring='roc_auc', cv=5, verbose=1, n_jobs=-1)\n# search.fit(X_train_split, y_train_split)\n\n# # Using the best parameters found\n# xgb_model_best = search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.134919Z","iopub.execute_input":"2023-10-02T16:16:39.135638Z","iopub.status.idle":"2023-10-02T16:16:39.146395Z","shell.execute_reply.started":"2023-10-02T16:16:39.135596Z","shell.execute_reply":"2023-10-02T16:16:39.145220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best_parameters = search.best_params_\n# print(best_parameters)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.148201Z","iopub.execute_input":"2023-10-02T16:16:39.149015Z","iopub.status.idle":"2023-10-02T16:16:39.161147Z","shell.execute_reply.started":"2023-10-02T16:16:39.148969Z","shell.execute_reply":"2023-10-02T16:16:39.160185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Predict on the validation set using the best estimator\n# y_pred_best_val = search.best_estimator_.predict(X_val_split)\n\n# # Get prediction probabilities for AUC score using the best estimator\n# y_pred_prob_best_val = search.best_estimator_.predict_proba(X_val_split)[:, 1]\n\n# from sklearn.metrics import accuracy_score, roc_auc_score\n\n# # Calculate accuracy\n# accuracy_best_val = accuracy_score(y_val_split, y_pred_best_val)\n\n# # Calculate ROC AUC\n# roc_auc_best_val = roc_auc_score(y_val_split, y_pred_prob_best_val)\n\n# print(f\"Optimized XGBoost Validation Accuracy: {accuracy_best_val:.2f}\")\n# print(f\"Optimized XGBoost Validation AUC: {roc_auc_best_val:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.162816Z","iopub.execute_input":"2023-10-02T16:16:39.163241Z","iopub.status.idle":"2023-10-02T16:16:39.174159Z","shell.execute_reply.started":"2023-10-02T16:16:39.163209Z","shell.execute_reply":"2023-10-02T16:16:39.173327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Best Parameters","metadata":{}},{"cell_type":"code","source":"xgb_model_best = XGBClassifier(\n    objective='binary:logistic',\n    n_estimators=500,\n    max_depth=3,\n    learning_rate=0.001,\n    subsample=1,\n    colsample_bytree=0.7,\n    reg_lambda=0.001,\n    reg_alpha=1.0,\n    random_state=64\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:16:39.175521Z","iopub.execute_input":"2023-10-02T16:16:39.176493Z","iopub.status.idle":"2023-10-02T16:16:39.194170Z","shell.execute_reply.started":"2023-10-02T16:16:39.176449Z","shell.execute_reply":"2023-10-02T16:16:39.193378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Ensemble by Random Seed Mode","metadata":{"_uuid":"0bc59aad-1d15-4f14-ac24-7e5689b4990b","_cell_guid":"24b29992-ef87-434f-b642-a03e72dae8b5","trusted":true}},{"cell_type":"code","source":"# Set up number of random seeds\nn_seeds = 100\nall_preds = []\n\ntest_data_filtered = test_data[features]\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:24:50.272231Z","iopub.execute_input":"2023-10-02T16:24:50.274576Z","iopub.status.idle":"2023-10-02T16:24:50.287763Z","shell.execute_reply.started":"2023-10-02T16:24:50.274509Z","shell.execute_reply":"2023-10-02T16:24:50.286541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For each seed, train the model and predict outcomes on test set\nfor seed in range(n_seeds):\n    # Split train_data into training and validation sets\n    X_train_split, _, y_train_split, _ = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n\n    # Train the model on the training subset\n    xgb_model_best.fit(X_train_split, y_train_split)\n\n    # Predict on the test_data using the model\n    y_pred_test = xgb_model_best.predict(test_data_filtered)\n    \n    all_preds.append(y_pred_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:24:51.158653Z","iopub.execute_input":"2023-10-02T16:24:51.159431Z","iopub.status.idle":"2023-10-02T16:25:40.427898Z","shell.execute_reply.started":"2023-10-02T16:24:51.159389Z","shell.execute_reply":"2023-10-02T16:25:40.426681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert list of predictions to a numpy array\nall_preds_array = np.array(all_preds)\n\n# Calculate the mode for each passenger\nfinal_preds, _ = mode(all_preds_array, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:25:48.672158Z","iopub.execute_input":"2023-10-02T16:25:48.672599Z","iopub.status.idle":"2023-10-02T16:25:48.692595Z","shell.execute_reply.started":"2023-10-02T16:25:48.672568Z","shell.execute_reply":"2023-10-02T16:25:48.691593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['Survived'] = final_preds[0]\noutput = pd.DataFrame({'PassengerID': test_data['PassengerId'], 'Survived': final_preds[0]})\noutput.set_index(\"PassengerID\", inplace=True)\noutput.to_csv('submission.csv')\noutput","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:25:49.983839Z","iopub.execute_input":"2023-10-02T16:25:49.984531Z","iopub.status.idle":"2023-10-02T16:25:50.005871Z","shell.execute_reply.started":"2023-10-02T16:25:49.984471Z","shell.execute_reply":"2023-10-02T16:25:50.004901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What are the best features?\nThese results are from the final random seed, not the entire ensemble","metadata":{}},{"cell_type":"code","source":"X_train_filtered = X_train[features]\n\n# Initialize SHAP explainer. \nexplainer = shap.TreeExplainer(xgb_model_best)\n\n# Calculate SHAP values for a particular dataset. \nshap_values = explainer.shap_values(X_train_filtered)\n\n# Visualize the SHAP values for a specific instance \nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0, :], X_train_filtered.iloc[0, :])","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:26:48.213426Z","iopub.execute_input":"2023-10-02T16:26:48.213879Z","iopub.status.idle":"2023-10-02T16:26:48.604983Z","shell.execute_reply.started":"2023-10-02T16:26:48.213847Z","shell.execute_reply":"2023-10-02T16:26:48.603510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shap.summary_plot(shap_values, X_train_filtered)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:26:56.705162Z","iopub.execute_input":"2023-10-02T16:26:56.705556Z","iopub.status.idle":"2023-10-02T16:26:57.769293Z","shell.execute_reply.started":"2023-10-02T16:26:56.705529Z","shell.execute_reply":"2023-10-02T16:26:57.767875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get feature importances from the model\nfeature_importances = xgb_model_best.feature_importances_\n\n# Pair each feature with its importance score\nfeatures_with_scores = list(zip(features, feature_importances))\n\n# Sort the pairs by importance score in descending order\nsorted_features_with_scores = sorted(features_with_scores, key=lambda x: x[1], reverse=True)\n\n# Display the sorted features and their scores\nfor feature, score in sorted_features_with_scores:\n    print(f\"Feature: {feature}, F Score: {score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-02T16:27:03.506599Z","iopub.execute_input":"2023-10-02T16:27:03.507009Z","iopub.status.idle":"2023-10-02T16:27:03.517473Z","shell.execute_reply.started":"2023-10-02T16:27:03.506978Z","shell.execute_reply":"2023-10-02T16:27:03.516256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Next Steps\n* Figure out why my anaconda notebook is generating a higher score than this Kaggle notebook\n* Try setting stumps early and then building the model from there with step-size tuning\n* Try [HyperOpt](http://hyperopt.github.io/hyperopt/) instead to fine tune\n* Use [YellowBrick](https://www.scikit-yb.org/en/latest/) to visualize details about XGBoost\n* Implement [XGBFIR](https://github.com/limexp/xgbfir) to develop a better understanding of feature\n","metadata":{}}]}